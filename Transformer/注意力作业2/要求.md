# 要求

设计一个包含注意力的简单语言模型，要求如下:

- 模型结构: 
    1. 包含因果注意力掩码的注意力层
    2. 多层感知机全连接层
- 训练要求:
    - 输入: `[CLS] 我 爱 中 国 ！`
    - 输出: `我 爱 中 国 ！[SEP]`

```python
from sentence_transformers import SentenceTransformer

embedding = SentenceTransformer('bge-small-zh')
# token 转 索引
embedding.tokenizer.convert_tokens_to_ids(['[CLS]', '我', '爱', '中国', '！', '[SEP]'])
```
