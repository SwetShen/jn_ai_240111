# 要求

设计一个包含注意力的简单语言模型，要求如下:

- 模型结构:
    1. 包含因果注意力掩码的注意力层
    2. 多层感知机全连接层
- 训练阶段要求:
    - 输入: `[CLS] 我 爱 中 国 ！`
    - 输出: `我 爱 中 国 ！[SEP]`
- 测试阶段要求:
    - 输入: `[CLS]`
    - 输出: `我`
    - 拼接输出的最后一个字到输入中
    - 输入: `[CLS] 我`
    - 输出: `我 爱`
    - 拼接输出的最后一个字到输入中
    - 输入: `[CLS] 我 爱`
    - 循环直到输出 `[SEP]` 为止

```python
from sentence_transformers import SentenceTransformer

embedding = SentenceTransformer('bge-small-zh')
# token 转 索引
embedding.tokenizer.convert_tokens_to_ids(['[CLS]', '我', '爱', '中国', '！', '[SEP]'])
```
