## 机器学习模型评估方法



### 一、混淆矩阵

要理解AUC、PR曲线、ROC曲线，必须看看懂混淆矩阵

| 真假/阴阳    | 0（Negative）              | 1（Positive）              |
| -------- | ------------------------ | ------------------------ |
| 0（Flase） | TN（True Negative：正确阴性）   | FP（Flase Positive：错误的阳性） |
| 1（True）  | FN（False Negative：错误的阴性） | TP（True Positive：正确的阳性）  |



+ TP：预测为正样本，实际也是正样本

+ FP：预测为正样本，实际是负样本

+ FN：预测为负样本，实际是正样本

+ TN：预测为负样本，实际也是负样本。

+ P = TP + FN ：所有”实际为正例“的样本数

+ N = FP + TN ： 所有”实际为负例“的样本数

+ P~=TP + FP ：所有”“预测为正例”的样本数

+ N~=TN+FN ：所有“预测为负例”的样本数



## 二、评价函数



### 1、ROC曲线 & AUC



**灵敏度（真阳率）**

正确阳性 / （混淆矩阵第一列）

$$
TPR = \frac{TP}{TP+FN} = \frac{TP}{P}
$$

含义：TP除以第一列，即预测为1实际为1的样本在所有真实1类别中的占比



**特异度（假阳率）**

错误阳性 / （混淆矩阵第二列）

$$
FPR=\frac{FP}{FP+TN}=\frac{FP}{N}
$$

含义：FP除以第二列，即预测为1实际为0的样本在所有真实0类别中的占比



ROC（Receiver Operating Charateristic Curve） 受试者工作曲线特征

在模型预测的时候，我们输出的预测结果是一堆[0-1]之间的数值，怎么把所有数值变成二分类？设置一个阈值，大于这个阈值的分类为1，小于这个阈值的分类为0。ROC曲线就是我们从[0,1]设置一堆阈值，每个阈值得到一个（TPR，FPR）对，纵轴为TPR，横轴为FPR，把所有的（TPR，FPR）对连起来就得到了ROC曲线。



### 2、PR曲线 & AP



**Recall召回率（查全率）**

正确阳性 / （混淆矩阵第一列）

$$
Recall = \frac{TP}{TP + FN}
$$



含义：TPTP除以第一列，即预测为1实际为1的样本在所有真实为1类别中的占比。



**Precision精准率（查准率）**

正确阳性/（混淆矩阵第一行）

$$
Precision = \frac{TP}{TP+FP}
$$



含义：FP除以第一行，即预测为1实际为1的样本在所有预测为1类别中的占比。



在模型预测的时候，我们输出的预测结果是一堆[0,1]之间的数值，怎么把数值变成二分类？设置一个阈值，大于这个阈值的值分类为1，小于这个阈值的值分类为0。ROC曲线就是我们从[0,1]设置一堆阈值，每个阈值得到一个（Precision，Recall）对，纵轴为Precision，横轴为Recall，把所有的（Precision，Recall）对对连起来就得到了PR曲线。



### 3、AUC/AP等评价值



**AUC(area under the curve)：** 

（1）计算方法一 AUC即ROC曲线下的面积。曲线越靠近左上角，意味着TPR>FPR，模型的整体表现也就越好。所以我们可以断言，ROC曲线下的面积越大，模型效果越好。 最坏的情况是，总是有TPR=FPR，如下图，表示对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的。换句话说，分类器对于正例和负例毫无区分能力。如果AUC小于0.5，那么只要把预测类别取反，便得到了一个AUC大于0.5的分类器。

（2）计算方法二 AUC还有一种解释就是任取一对正负样本，正样本的预测值大于负样本的预测值的概率。 显然可以写出计算AUC伪代码为： 1.统计所有正样本个数P，负样本个数N； 2.遍历所有正负样本对，统计正样本预测值大于负样本预测值的样本总个数number 3.AUC = number / (P * N) 一些计算细节是当正负样本预测值刚好相等时，该样本记为0.5个。 PS: 在实际代码实现过程中其实可以和第一种方法一样进一步优化，可以先对正负样本排序，利用dp的思想迭代计算个数，可以将复杂度从$O(N^2)$降低为$O(NlogN)$。



---





**AP（PR曲线下的面积）值**：

跟TPR和FPR不一样的是，在PR关系中，是一个此消彼长的关系，但往往我们希望二者都是越高越好，所以PR曲线是右上凸效果越好（也有例外，有比如在风险场景当预测为1实际为0时需要赔付时，大致会要求Recall接近100%，可以损失Precision）。所以除了特殊情况，通常情况都会使用Precision-recall曲线，来寻找分类器在Precision与Recall之间的权衡。  
AP就是Precision-recall 曲线下面的面积，通常来说一个越好的分类器，AP值越高。  
还有一个mAP的概念，mAP是多个类别AP的平均值。这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP的大小一定在[0,1]区间，越大越好。该指标是目标检测算法中最重要的一个。  


***目标检测AP评估***  
**Confidence Score** **置信度分数**是一个分类器(Classifier)预测一个锚框(Anchor Box)中包含某个对象的概率(Probability)。通过设置Confidence Threshold置信度阈值可以过滤掉(不显示)小于threshold的预测对象。锚框的详细解释请参见《深度学习目标检测算法中的锚框(Anchor Box)是什么？》  
**IoU (Intersection over union)交并比，**预测框(Prediction)与原标记框(Ground truth)之间的重叠度(Overlap)，最理想情况是完全重叠，即比值为1。IoU用于衡量预测框的准确度。



在目标检测算法中，当一个检测结果(detection)被认为是True Positive时，需要同时满足下面三个条件：  
1，Confidence Score > Confidence Threshold  
2，预测类别匹配(match)真实值(Ground truth)的类别  
3，预测边界框(Bounding box)的IoU大于设定阈值，如0.5  
不满足条件2或条件3，则认为是False Positive。



**mAP(IoU@[0.5:0.05:0.95])**，需要计算10个IoU阈值下的mAP，然后计算平均值。这个评估指标比仅考虑通用IoU阈值(0.5)评估指标更能体现出模型的精度。  
**mAP(IoU@0.75)**，这是一个对检测能力要求更高的标准。  
除了根据不同的IoU阈值来计算mAP外，还可以根据检测目标的大小来计算。  
**mAP@small**，检测目标的面积 ≤ 32x32  
**mAP@medium**，32x32 ＜ 检测目标的面积 ≤ 96x96  
**mAP@Large**，96x96 ＜ 检测目标的面积



**F1 值：** 为了能够评价不同算法的优劣，在Precision和Recall的基础上提出了F1值的概念，来对Precision和Recall进行整体评价。F1的定义如下：

F1值 = 正确率 * 召回率 * 2 / （正确率 + 召回率）

$$
F_1 = (\frac{recall^{-1} + precision^{-1}}{2})^{-1} = 2 \cdot \frac{precision \cdot recall}{precition + recall}
$$

正是$ \beta $的通式为：

$$
F_{\beta} = (1 + \beta^2) \cdot \frac{precision \cdot recall}{\beta^{2} \cdot precision + recall}
$$





---



### ROC曲线、PR曲线的优缺点

**ROC曲线：**  
（1）优点  
A.兼顾正例和负例的权衡。因为TPR聚焦于正例，FPR聚焦于与负例，使其成为一个较均衡评估方法。适用于评估分类器的整体性能。  
B.ROC曲线两个指标， TPR的分母是所有正例，FPR的分母是所有负例，故都不依赖于具体类别分布。  
（2）缺点  
在类别不平衡的背景下，当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。 如果主要关心正例的预测准确性的话，这就不太可接受了。  


**ROC曲线：**  
（1）优点  
PR曲线的两个指标都聚焦于正例。类别不平衡问题中由于主要关心正例，所以在此情况下PR曲线被广泛认为优于ROC曲线。  
（2）缺点  
只关注正例，不关注负例  
ROC&PR曲线使用场景  
（1）如果想兼顾正例与负例，则选用ROC曲线；如果在类别不平衡中，或者更看重正例的场景中比如推荐信息检索，则选用PR曲线  
（2）如果有多份数据且存在不同的类别分布，比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想测试不同类别分布下对分类器的性能的影响，则PR曲线比较适合。：  



---



### Accuracy准确率

（混淆矩阵对角线）/所有  
含义：指的是预测正确的样本数量占总量的百分比

$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN} =\frac{TP+TN}{P + N}
$$

accuracy首先是correct（正确），而precision首先是exact（精确，或者说确切）。首先准确，然后才谈得上精确。一个结果必须要同时符合准确与精密这两个条件，才可算是精准。  
准确率有一个缺点，就是数据的样本不均衡，这个指标是不能评价模型的性能优劣的。  
这两个词也 有点类似 偏差（bias）与方差（variance）  
偏差（bias）反映了模型在样本上的期望输出与真实标记之间的差距，即模型本身的精准度，反映的是模型本身的拟合能力。这就很像 Precision。  
方差（variance）反映了模型在不同训练数据集下学得的函数的输出与期望输出之间的误差，即模型的稳定性，反应的是模型的波动情况。这有点像 Accuracy。  
比如大概可以类比成咱射箭，准确率要看你射中靶心的概率；精准率要看你射中的是靶心区域的哪个位置。


